# -*- coding: utf-8 -*-
"""Untitled24.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nfiNO6ffaAJwimrwRzCzIoYbXlOUGwxz
"""

import torch
from torch import nn
import pathlib
from torch.utils.data import DataLoader
from torchvision import *
import torch.nn.functional as F
import torch.optim as optim
from PIL import Image
import os
import time
import copy
import torchvision
import csv
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

data_transforms = {
	'train': transforms.Compose([
		transforms.Resize((20,20), interpolation=2),
		transforms.ToTensor()
	]),
	'val': transforms.Compose([
		transforms.Resize((20,20), interpolation=2),
		transforms.ToTensor()
	])
}

data_dir = '/content/drive/MyDrive/gnr638/gnr638_train/'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64, shuffle=True, num_workers=2) for x in ['train']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train']}
class_names = image_datasets['train'].classes

#image, label = next(iter(dataloaders['train']))

def train_model(model, criterion, optimizer, num_epochs=100):
	# print("hi",class_names)
	
	for epoch in range(num_epochs):
		print('Epoch {}/{}'.format(epoch, num_epochs - 1))
		print('-' * 10)
		

		loss = 0
    model.train()
		for inputs, labels in dataloaders['train']:
			inputs = inputs.to(device)
			labels = labels.to(device)
			optimizer.zero_grad()
			outputs = model(inputs)
			loss = criterion(outputs, labels)
			loss.backward()
			optimizer.step()
			loss += loss.item() * inputs.size(0)
			epoch_loss = loss / dataset_sizes['train']
			print('{} Loss: {:.4f}'.format('train', epoch_loss))
			torch.save(model, 'model.pkl')
   
   with toch.no_grad():
     model.eval()
     inputs = inputs.to(device)
			labels = labels.to(device)
			optimizer.zero_grad()
			outputs = model(inputs)
			loss = criterion(outputs, labels)
			loss += loss.item() * inputs.size(0)
			epoch_loss = loss / dataset_sizes['train']
			print('{} Loss: {:.4f}'.format('train', epoch_loss))
			torch.save(model, 'model_kaggle.pkl')
	return model

class Net(nn.Module):
	def __init__(self):
		super().__init__()
		self.conv1 = c
		self.conv2 = nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1)
		self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
		
		self.conv3 = nn.Conv2d(64,128, kernel_size=3, stride=1, padding=1)
		self.conv4 = nn.Conv2d(128,256, kernel_size=3, stride=1, padding=1)
		self.conv5 = nn.Conv2d(256,512, kernel_size=3, stride=1, padding=1)
		self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) #512*x1*x2 - 1d, .view(-1) 
		
		self.fc1 = nn.Linear(512*5*5, 1024) #classifier 
		self.drop_layer1 = nn.Dropout(p=0.25)
		self.fc2 = nn.Linear(1024,256)
		self.drop_layer2 = nn.Dropout(p=0.25)
		self.fc3 = nn.Linear(256,3)
    self.encoder = nn.Sequential(
        nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1),
        nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1),

    )

	def forward(self, xb):
		xb = F.relu(self.conv1(xb))
		xb = F.relu(self.conv2(xb))
		xb = self.pool2(xb)
		xb = F.relu(self.conv3(xb))
		xb = F.relu(self.conv4(xb))
		xb = F.relu(self.conv5(xb))
		xb = self.pool3(xb)
		xb = xb.reshape(xb.shape[0], 512*5*5)
		xb = F.relu(self.fc1(xb))
		xb = self.drop_layer1(xb)
		xb = F.relu(self.fc2(xb))
		xb = self.drop_layer2(xb)
		xb = self.fc3(xb)
		return xb



model = Net().to(device)
opt = optim.Adam(model.parameters(), lr=1e-4)
error = nn.CrossEntropyLoss()   
train_model= train_model(model, error, opt)

model = torch.load("model.pkl")
model.train()